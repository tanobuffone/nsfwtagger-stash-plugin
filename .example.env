# NSFWTagger AI Plugin - Environment Configuration
# Simplified configuration for Ollama integration only
# Copy this file to .env and update the values as needed

# ===========================================
# OLLAMA CONFIGURATION
# ===========================================

# Ollama API endpoint
# Default: http://localhost:11434
OLLAMA_HOST=http://localhost:11434

# Primary vision model for content analysis
# Default: qwen3-vl-4b-instruct/Qwen3-VL-4B-Instruct-Q4_K_M.gguf
OLLAMA_MODEL=qwen3-vl-4b-instruct/Qwen3-VL-4B-Instruct-Q4_K_M.gguf

# Fallback model for OCR and text analysis
# Default: olmocr-2-7b-1025/olmOCR-2-7B-1025-Q4_K_M.gguf
OLLAMA_FALLBACK_MODEL=olmocr-2-7b-1025/olmOCR-2-7B-1025-Q4_K_M.gguf

# ===========================================
# PROCESSING CONFIGURATION
# ===========================================

# Frame extraction interval in seconds
# How often to extract frames from videos
# Default: 10 (every 10 seconds)
FRAME_INTERVAL=10

# Maximum frames to extract per video
# Default: 25
MAX_FRAMES_PER_VIDEO=25

# Number of concurrent processing tasks
# Default: 2
CONCURRENCY=2

# Number of items per processing batch
# Default: 5
BATCH_SIZE=5

# ===========================================
# LOGGING CONFIGURATION
# ===========================================

# Logging level: DEBUG, INFO, WARNING, ERROR
# Default: INFO
LOG_LEVEL=INFO

# ===========================================
# EXAMPLE CONFIGURATIONS
# ===========================================

# High Performance (fast processing, more resources)
# FRAME_INTERVAL=5
# MAX_FRAMES_PER_VIDEO=50
# CONCURRENCY=4
# BATCH_SIZE=10

# Memory Efficient (slower but uses less RAM)
# FRAME_INTERVAL=15
# MAX_FRAMES_PER_VIDEO=15
# CONCURRENCY=1
# BATCH_SIZE=3

# Balanced (recommended for most users)
# FRAME_INTERVAL=10
# MAX_FRAMES_PER_VIDEO=25
# CONCURRENCY=2
# BATCH_SIZE=5

# ===========================================
# TROUBLESHOOTING TIPS
# ===========================================

# If you get timeout errors:
# - Reduce CONCURRENCY
# - Increase BATCH_DELAY (if available)

# If AI analysis is slow:
# - Reduce CONCURRENCY
# - Reduce MAX_FRAMES_PER_VIDEO

# If you run out of memory:
# - Reduce CONCURRENCY
# - Reduce MAX_FRAMES_PER_VIDEO

# If you get connection errors:
# - Verify OLLAMA_HOST is correct
# - Check if Ollama is running
# - Verify models are loaded

# For debugging:
# - Set LOG_LEVEL=DEBUG
# - Check Docker container logs
